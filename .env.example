# ============================================
# RAG API Server Configuration
# ============================================
# Copy this file to .env and fill in your values
# NEVER commit .env to version control!

# --------------------------------------------
# OpenAI Configuration (REQUIRED)
# --------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Custom OpenAI-compatible API endpoint
# Leave blank to use default OpenAI API (https://api.openai.com)
# Example for custom endpoint: https://api.bltcy.ai
OPENAI_API_BASE=

# --------------------------------------------
# Database Configuration (REQUIRED)
# --------------------------------------------
# PostgreSQL connection string with pgvector extension
# Format: postgresql://user:password@host:port/database
# Local: postgresql://postgres:postgres@localhost:5432/llamaindex_rag
# Railway: Automatically provided as DATABASE_URL (rename to DB_URL)
DB_URL=postgresql://postgres:postgres@localhost:5432/llamaindex_rag

# --------------------------------------------
# Model Configuration
# --------------------------------------------
# OpenAI embedding model for document vectorization
# Options: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
EMBEDDING_MODEL=text-embedding-3-small

# OpenAI chat model for response generation
# Options: gpt-4, gpt-4-turbo, gpt-3.5-turbo, or custom models
CHAT_MODEL=gpt-4.1-mini

# --------------------------------------------
# Application Configuration
# --------------------------------------------
# Maximum number of conversation messages to include in context
# Higher = more context but slower and more expensive
# Recommended: 10-20
MAX_HISTORY_MESSAGES=10

# Default number of document chunks to retrieve for RAG
# Higher = more context but slower
# Recommended: 3-10
TOP_K_DEFAULT=5

# Default temperature for LLM responses (0.0 = deterministic, 2.0 = creative)
# Recommended: 0.3-0.7
DEFAULT_TEMPERATURE=0.3

# --------------------------------------------
# Server Configuration
# --------------------------------------------
# Server host (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
HOST=0.0.0.0

# Server port (Railway sets this automatically via PORT env var)
PORT=8000

# API version
VERSION=0.1.0

# --------------------------------------------
# Production Configuration
# --------------------------------------------
# Environment: development, staging, production
# Affects logging, error handling, and security settings
ENVIRONMENT=development

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Development: DEBUG or INFO
# Production: INFO or WARNING
LOG_LEVEL=INFO

# CORS origins (comma-separated list of allowed origins)
# Development: http://localhost:3000,http://localhost:5173
# Production: Specific origins only (e.g., https://app.example.com,https://admin.example.com)
# IMPORTANT: When using credentials, you CANNOT use * - must specify exact origins
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# Maximum request body size in bytes
# Default: 10MB (10485760 bytes)
# Adjust based on expected document sizes
MAX_REQUEST_SIZE=10485760

# Request timeout in seconds
# How long to wait for OpenAI API responses
# Recommended: 60-120 seconds
REQUEST_TIMEOUT=60

# Number of Uvicorn worker processes
# Development: 1
# Production: 2-4 (formula: 2 Ã— CPU cores + 1)
# More workers = handle more concurrent requests
WORKERS=1
